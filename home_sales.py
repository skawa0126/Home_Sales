# -*- coding: utf-8 -*-
"""Home_Sales.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y8tMJcu6KGngLoTbYUO2jgmCxRcTrXWZ
"""

! pip install findspark
# Import findspark and initialize.
import findspark
findspark.init()

# Import packages
from pyspark.sql import SparkSession
import time

# Create a SparkSession
spark = SparkSession.builder.appName("SparkSQL").getOrCreate()

# 1. Read in the AWS S3 bucket into a DataFrame.
from pyspark import SparkFiles
url = "https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv"

spark.sparkContext.addFile(url)
homesales = spark.read.option('header', 'true').csv(SparkFiles.get("home_sales_revised.csv"), inferSchema=True, sep=',', timestampFormat="mm/dd/yy")

# Show DataFrame
homesales.show()

# 2. Create a temporary view of the DataFrame.
homesales.createOrReplaceTempView('homes')
spark.sql("SELECT * FROM homes LIMIT 10").show()

# 3. What is the average price for a four bedroom house sold per year, rounded to two decimal places?
query = f"""
(SELECT
  YEAR(date), ROUND(AVG(price),2) FROM homes
  WHERE bedrooms==4
  GROUP BY YEAR(date)
  ORDER BY YEAR(date) ASC
  )
"""
spark.sql(query).show()

# 4. What is the average price of a home for each year the home was built,
# that have 3 bedrooms and 3 bathrooms, rounded to two decimal places?
query = f"""
(SELECT
  YEAR(date), ROUND(AVG(price),2) FROM homes
  WHERE bedrooms==3
    AND bathrooms==3
  GROUP BY YEAR(date)
  ORDER BY YEAR(date) ASC
  )
"""
spark.sql(query).show()

# 5. What is the average price of a home for each year the home was built,
# that have 3 bedrooms, 3 bathrooms, with two floors,
# and are greater than or equal to 2,000 square feet, rounded to two decimal places?
query = f"""
(SELECT
  YEAR(date), ROUND(AVG(price),2) FROM homes
  WHERE bedrooms==3
    AND bathrooms==3
    AND floors==2
    AND sqft_living>=2000
  GROUP BY YEAR(date)
  ORDER BY YEAR(date) ASC
  )
"""
spark.sql(query).show()

# 6. What is the average price of a home per "view" rating, rounded to two decimal places,
# having an average home price greater than or equal to $350,000? Order by descending view rating.
# Although this is a small dataset, determine the run time for this query.

start_time = time.time()

query = f"""
(SELECT
  view, ROUND(AVG(price),2) FROM homes
  GROUP BY view
  HAVING ROUND(AVG(price),2) >= 350000
  ORDER BY view DESC
  )
"""
spark.sql(query).show()

print("--- %s seconds ---" % (time.time() - start_time))

# 7. Cache the the temporary table home_sales.
spark.sql("cache table homes")

# 8. Check if the table is cached.
spark.catalog.isCached('homes')

# 9. Using the cached data, run the last query above, that calculates
# the average price of a home per "view" rating, rounded to two decimal places,
# having an average home price greater than or equal to $350,000.
# Determine the runtime and compare it to the uncached runtime.

start_time = time.time()

spark.sql("SELECT view, ROUND(AVG(price),2) from homes group by view having ROUND(AVG(price),2) >= 350000").show()

print("--- %s seconds ---" % (time.time() - start_time))

# 10. Partition by the "date_built" field on the formatted parquet home sales data
homesales.write.partitionBy("date_built").mode("overwrite").parquet("parquet_homes")

# 11. Read the formatted parquet data.
parquet_df = spark.read.parquet('parquet_homes')

# 12. Create a temporary table for the parquet data.
parquet_df.createOrReplaceTempView('parquetdata')
spark.sql("SELECT * FROM parquetdata LIMIT 10").show()

# 13. Using the parquet DataFrame, run the last query above, that calculates
# the average price of a home per "view" rating, rounded to two decimal places,
# having an average home price greater than or equal to $350,000.
# Determine the runtime and compare it to the cached runtime.

start_time = time.time()

spark.sql("SELECT view, ROUND(AVG(price),2) from parquetdata group by view having ROUND(AVG(price),2) >= 350000").show()

print("--- %s seconds ---" % (time.time() - start_time))

# 14. Uncache the home_sales temporary table.
spark.sql("uncache table homes")

# 15. Check if the home_sales is no longer cached
spark.catalog.isCached('homes')

